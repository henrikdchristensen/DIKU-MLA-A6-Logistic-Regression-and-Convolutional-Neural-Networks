{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic regression in PyTorch (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are supposed to fill in the missing lines in the notebook `Logistic Regression PyTorch Assignment.ipynb`. The tutorial https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html should provide you with all knowledge you need.\n",
    "\n",
    "The notebook considers an example where 2D data points are classified using logistic regression and the solution is visualized. First, an implementation in Scikit-Learn is given. Then the same is partly implemented using PyTorch. Your assignment is to fill in the blanks in the PyTorch implementation.\n",
    "\n",
    "You should add some code in several cells (indicated by \"MISSING\"). Please write these lines of code in your report, see the LATEX template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Sobel filter (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you should implement a Sobel filter in PyTorch. The learning goals are a better understanding of basic image filtering operations, a deep understanding of how the PyTorch convolutional layers work, and better PyTorch programming skills.\n",
    "\n",
    "The Sobel filter is a basic operator to highlight edges in images (e.g., see Wikipedia). It convolves the image with two 3×3 filters, the results of which are combined. Let the matrix I represent a (single-channel) 2D input image and ∗ the convolution operator. Then the Sobel filter computes in a first step\n",
    "$$\n",
    "\\mathbf{G}_x =\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & -1 \\\\\n",
    "        2 & 0 & -2 \\\\\n",
    "        1 & 0 & -1\n",
    "    \\end{bmatrix} \\ast \\mathbf{I}\n",
    "    \n",
    "    \\quad \\text{and} \\quad\n",
    "    \n",
    "    \\mathbf{G}_y =\n",
    "    \\begin{bmatrix}\n",
    "        1 & 2 & 1 \\\\\n",
    "        0 & 0 & 0 \\\\\n",
    "        -1 & -2 & -1\n",
    "    \\end{bmatrix} \\ast \\mathbf{I},\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "which are then combined to a feature map\n",
    "$$\n",
    "[\\mathbf{G}]_{ij} = \\sqrt{[\\mathbf{G}_x]_{ij}^2 + [\\mathbf{G}_y]_{ij}^2}\n",
    "\\tag{2}\n",
    "$$\n",
    "where $[\\cdot]_{ij}$ denotes the matrix element at position $ij$. Start by going through the notebook `PyTorch 2D convolutions.ipynb`, which gives some examples on how to work with 2D convolutions in PyTorch and provides some basics on how to read and display images. Do this carefully. First, it is surprising how many people doing deep learning in Python actually do not know what is going in a simple convolutional layer. Second, handling input images in real-world applications can sometimes be tricky (and I am not claiming that the way I did it in the notebook is the most elegant one).\n",
    "\n",
    "Implement the Sobel filter. Use a single `nn.Conv2d` call to generate the two feature maps $\\mathbf{G}_x$ and $\\mathbf{G}_y$. For squaring each element in a filter output you can use something like torch.square. For combining filter outputs as in (2) you can use `torch.sum`, but you have to set the axis argument properly.\n",
    "\n",
    "Deliverables: Show the important parts of your code (i.e., the Sobel filter) in the report. Apply the Sobel filter to the same input image as in `PyTorch 2D convolutions.ipynb`. Deliverables: Show plots of the two feature maps and the final result (i.e., $\\mathbf{G}_x$, $\\mathbf{G}_y$, and $\\mathbf{G}$).\n",
    "\n",
    "Deliverables: In the report, discuss how you considered the difference between convolution and cross-correlation (note that the latter is what the \"convolutional\" layer is doing) in your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Convolutional neural networks (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning goal of this part of the assignment is to get more comfortable with implementing convolutional neural networks (CNNs) in PyTorch.\n",
    "\n",
    "Consider the notebook `Torch_Traffic_Signs_Basic_Template.ipynb` and the file `GTSRBTrafficSigns.py`. The notebook is a template for training a convolutional neural network (CNN) for classifying traffic signs on the same data as the competition described by Stallkamp et al. (2012). The data is handled by the Dataset class in the file `GTSRBTrafficSigns.py`. Recognition of traffic signs is a challenging real-world problem of high industrial relevance. Traffic sign recognition can be viewed as a multi-class classification problem with unbalanced class frequencies, in which one has to cope with large variations in visual appearances due to illumination changes, partial occlusions, rotations, weather conditions, etc. However, humans are capable of recognizing the large variety of existing road signs with close to 100% correctness – not only in real-world driving situations, which provides both context and multiple views of a single traffic sign, but also when looking at single images as those shown in Figure 1. Now the question is how good a computer can become at solving this problem.\n",
    "\n",
    "The notebook is supposed to run with GPU support, for example, using _Google Colaboratory_. Even then, executing it will take some time.\n",
    "\n",
    "The notebook misses the CNN definition. Define a network with a\n",
    "- convolutional layer creating 64 feature maps using 5×5 kernels followed by a\n",
    "- 2 × 2 max-pooling layer followed by a\n",
    "- convolutional layer creating 64 feature maps using 5×5 kernels followed by a\n",
    "- 2 × 2 max-pooling layer followed by a\n",
    "- linear fully-connected layer with 43 outputs.\n",
    "\n",
    "After each convolutional layer, the ELU (exponential linear unit, Clevert et al. (2016)) activation function should be applied.\n",
    "\n",
    "Deliverables: Show the model definition in the report.\n",
    "You should get the following output when you print your model:\n",
    "``````\n",
    "    Net(\n",
    "        ( conv1 ): Conv2d (3, 64, kernel_size =(5, 5) , stride =(1, 1))\n",
    "        ( pool1 ): MaxPool2d ( kernel_size =2, stride =2, padding =0, dilation\n",
    "        =1, ceil_mode = False )\n",
    "        ( conv2 ): Conv2d (64 , 64, kernel_size =(5, 5) , stride =(1, 1))\n",
    "        ( pool2 ): MaxPool2d ( kernel_size =2, stride =2, padding =0, dilation\n",
    "        =1, ceil_mode = False )\n",
    "        (fc2): Linear ( in_features =1024 , out_features =43, bias = True )\n",
    "    )\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Augmentation (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are highly complex models. To reduce the risk of overfitting and to be able to learn difficult tasks with a high input variability, many training examples are needed. Data augmentation is a way to enlarge the training data set by artificial training points generated from the available data. Data augmentation refers to applying transformations to the input data (the images) that do not change their labels. If we know, for example, that the classifications of images do not change when the images are rotated, then showing rotated versions of the images during the training process helps to learn this invariance property. In the context of neural networks, this type of data augmentation can be traced back to Baird (1992). It has been used successfully for CNNs, for instance, already in the influential work by Krizhevsky et al. (2012).\n",
    "\n",
    "The learning goal of this part of the assignment is to get more experience in using data augmentation.\n",
    "\n",
    "Inspect the notebook `Torch_Traffic_Signs_Basic_Template.ipynb`.\n",
    "\n",
    "Deliverables: Answer the following questions briefly in your report: Which transformations are applied to the input images during training? Why is a transformation conditioned on the label?\n",
    "\n",
    "Please add at least one additional (not completely nonsensical) transformation.\n",
    "\n",
    "Deliverables: Show the corresponding code in your report and briefly argue why you think that this may be a reasonable augmentation given the task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
